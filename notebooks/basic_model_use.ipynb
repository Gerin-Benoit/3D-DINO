{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad714cc1-a8c7-49ec-abc9-268757356de3",
   "metadata": {},
   "source": [
    "# Basic 3DINO-ViT Usage Example\n",
    "\n",
    "This notebook shows a basic example of how the pretrained 3DINO-ViT model could be used to extract features from an input image. Change all paths to your local ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3d2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # adjust this to your local path\n",
    "from dinov2.eval.setup import build_model_for_eval\n",
    "from dinov2.configs import load_and_merge_config_3d\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f15a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'path_to_pretrained_weights'\n",
      "No weights found, using random initialization!\n",
      "DinoVisionTransformer3d(\n",
      "  (patch_embed): PatchEmbed3d(\n",
      "    (proj): Conv3d(1, 1024, kernel_size=(16, 16, 16), stride=(16, 16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): BlockChunk(\n",
      "      (0-5): 6 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BlockChunk(\n",
      "      (0-5): 6 x Identity()\n",
      "      (6-11): 6 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): BlockChunk(\n",
      "      (0-11): 12 x Identity()\n",
      "      (12-17): 6 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): BlockChunk(\n",
      "      (0-17): 18 x Identity()\n",
      "      (18-23): 6 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# use config and path to pretrained weights to load the pretrained 3DINO-ViT model\n",
    "config_file = 'train/vit3d_highres'\n",
    "pretrained_weights = 'path_to_pretrained_weights'  # adjust this to local path\n",
    "\n",
    "cfg = load_and_merge_config_3d(config_file)\n",
    "model = build_model_for_eval(cfg, pretrained_weights)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db800c8-5557-4bdc-b762-396d35955be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(-1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# the minimal preprocessing of the input image should be normalizing it to have values ranging between -1 and 1\n",
    "# shape is batch size, channels, and spatial dims\n",
    "example_img = torch.randn(1, 1, 112, 112, 112).cuda()\n",
    "\n",
    "# for example: \n",
    "# normalize 99.95% percentile to 1 and 0.05% percentile to -1, then clip to -1, 1\n",
    "min_val = torch.quantile(example_img, 0.0005)\n",
    "max_val = torch.quantile(example_img, 0.9995)\n",
    "example_img = (example_img - min_val) / (max_val - min_val)\n",
    "example_img = torch.clip(example_img * 2 - 1, -1, 1)\n",
    "\n",
    "print(example_img.max(), example_img.min())\n",
    "\n",
    "out = model(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d00214-1ed7-42c2-85c2-305ad0c21130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# output is a feature vector of size 1024\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6db6e0-6971-448d-936f-3cc06f516b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}